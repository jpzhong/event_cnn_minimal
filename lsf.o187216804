Sender: LSF System <lsfadmin@eu-g2-15>
Subject: Job 187216804: <python inference.py --device 0 --events_file_path /cluster/scratch/jzhong/fr_dataset/val/desk_fast.h5 --output_folder /cluster/scratch/jzhong/event_cnn_recon_upd --checkpoint_path pretrained/recon/update_reconstruction_model.pth> in cluster <euler> Exited

Job <python inference.py --device 0 --events_file_path /cluster/scratch/jzhong/fr_dataset/val/desk_fast.h5 --output_folder /cluster/scratch/jzhong/event_cnn_recon_upd --checkpoint_path pretrained/recon/update_reconstruction_model.pth> was submitted from host <eu-login-27> by user <jzhong> in cluster <euler> at Wed Oct  6 18:32:15 2021
Job was executed on host(s) <eu-g2-15>, in queue <gpu.4h>, as user <jzhong> in cluster <euler> at Wed Oct  6 18:32:38 2021
</cluster/home/jzhong> was used as the home directory.
</cluster/home/jzhong/event_cnn_minimal> was used as the working directory.
Started at Wed Oct  6 18:32:38 2021
Terminated at Wed Oct  6 18:32:56 2021
Results reported at Wed Oct  6 18:32:56 2021

Your job looked like:

------------------------------------------------------------
# LSBATCH: User input
python inference.py --device 0 --events_file_path /cluster/scratch/jzhong/fr_dataset/val/desk_fast.h5 --output_folder /cluster/scratch/jzhong/event_cnn_recon_upd --checkpoint_path pretrained/recon/update_reconstruction_model.pth
------------------------------------------------------------

Exited with exit code 1.

Resource usage summary:

    CPU time :                                   6.49 sec.
    Max Memory :                                 3326 MB
    Average Memory :                             336.00 MB
    Total Requested Memory :                     20000.00 MB
    Delta Memory :                               16674.00 MB
    Max Swap :                                   -
    Max Processes :                              3
    Max Threads :                                4
    Run time :                                   25 sec.
    Turnaround time :                            41 sec.

The output (if any) follows:

Loading checkpoint: pretrained/recon/update_reconstruction_model.pth ...
Using skip: <function skip_sum at 0x2b2bc60e3a70>
Using UpsampleConvLayer (slow, but no checkerboard artefacts)
Kernel size 5
Skip type sum
norm none
FlowNet(
  (unetflow): UNetFlow(
    (head): ConvLayer(
      (conv2d): Conv2d(10, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
    )
    (encoders): ModuleList(
      (0): RecurrentConvLayer(
        (conv): ConvLayer(
          (conv2d): Conv2d(32, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))
        )
        (recurrent_block): ConvLSTM(
          (Gates): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (1): RecurrentConvLayer(
        (conv): ConvLayer(
          (conv2d): Conv2d(64, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))
        )
        (recurrent_block): ConvLSTM(
          (Gates): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
      (2): RecurrentConvLayer(
        (conv): ConvLayer(
          (conv2d): Conv2d(128, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))
        )
        (recurrent_block): ConvLSTM(
          (Gates): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        )
      )
    )
    (resblocks): ModuleList(
      (0): ResidualBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
      (1): ResidualBlock(
        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
        (relu): ReLU(inplace=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      )
    )
    (decoders): ModuleList(
      (0): UpsampleConvLayer(
        (conv2d): Conv2d(256, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
      )
      (1): UpsampleConvLayer(
        (conv2d): Conv2d(128, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
      )
      (2): UpsampleConvLayer(
        (conv2d): Conv2d(64, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))
      )
    )
    (pred): ConvLayer(
      (conv2d): Conv2d(32, 3, kernel_size=(1, 1), stride=(1, 1))
    )
  )
)
Trainable parameters: 0
sensor resolution = [180 240]
Saving to: /cluster/scratch/jzhong/event_cnn_recon_upd
  0%|          | 0/723 [00:00<?, ?it/s]  0%|          | 0/723 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "inference.py", line 182, in <module>
    main(args, model)
  File "inference.py", line 104, in main
    output = model(voxel)
  File "/cluster/home/jzhong/miniconda3/envs/event_cnn/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/cluster/home/jzhong/event_cnn_minimal/model/model.py", line 138, in forward
    output_dict = self.unetflow.forward(event_tensor)
  File "/cluster/home/jzhong/event_cnn_minimal/model/unet.py", line 166, in forward
    x = self.head(x)
  File "/cluster/home/jzhong/miniconda3/envs/event_cnn/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/cluster/home/jzhong/event_cnn_minimal/model/submodules.py", line 26, in forward
    out = self.conv2d(x)
  File "/cluster/home/jzhong/miniconda3/envs/event_cnn/lib/python3.7/site-packages/torch/nn/modules/module.py", line 1051, in _call_impl
    return forward_call(*input, **kwargs)
  File "/cluster/home/jzhong/miniconda3/envs/event_cnn/lib/python3.7/site-packages/torch/nn/modules/conv.py", line 443, in forward
    return self._conv_forward(input, self.weight, self.bias)
  File "/cluster/home/jzhong/miniconda3/envs/event_cnn/lib/python3.7/site-packages/torch/nn/modules/conv.py", line 440, in _conv_forward
    self.padding, self.dilation, self.groups)
RuntimeError: Given groups=1, weight of size [32, 10, 5, 5], expected input[1, 5, 184, 240] to have 10 channels, but got 5 channels instead
== Timing statistics ==
Inference: 45.77 ms (1 samples)
